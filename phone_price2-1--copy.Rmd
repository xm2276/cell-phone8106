---
title: "Classification for phone prices"
author: "XIAO MA, HAO ZHENG, YONGZI YU"
date: "05/09/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.height = 8,
  fig.width = 12,
  dpi = 200, 
  message = F,
  echo = T,
  warning = F,
  cache = T
)
# theme_set(theme_minimal() + theme(
#   legend.position = "bottom",
#   plot.title = element_text(hjust = 0.5)
# ))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  digits = 3
)
# scale_colour_discrete = scale_colour_viridis_d
# scale_fill_discrete = scale_fill_viridis_d
```


```{r}
# library(reticulate)
library(caret)
library(tidyverse)
library(ggpubr)
library(doParallel)
library(ranger)
library(pROC)
library(gbm)
library(pdp)
library(lime)
library(cutpointr)
```

```{r}
# read data
df = read.csv("data/train.csv")

# covert outcome to binary
df$price_range = as.factor(ifelse(df$price_range >=2, "High", "Low"))

# convert data format
df = df %>% 
    mutate_at(vars("blue", "dual_sim", "four_g", "three_g", "touch_screen", "wifi"), 
              ~factor(., levels = c(0, 1), labels = c("No", "Yes")))
```


```{r data_preprocess}
# split into training set
set.seed(1)
train_index = createDataPartition(df$price_range,p=0.8,list = F)
train_df = df[train_index, ]
test_df = df[-train_index, ]
```


```{r}
# user parallel to accelarate 
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
```


## Bagging

```{r }
ctrl <- trainControl(method = "cv", classProbs = TRUE, 
                     summaryFunction = twoClassSummary) 

bagging.grid <- expand.grid(mtry = 20, # mtry = number of the predictors
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(1) 
bagging.fit <- train(price_range ~ . , 
                df, 
                subset = train_index, 
                tuneGrid = bagging.grid, 
                method = "ranger",
                metric = "ROC",
                trControl = ctrl)
```

```{r}
ggplot(bagging.fit, highlight = TRUE)
```

```{r}
# Permutation based importance
set.seed(1)
bagging.final.per <- ranger(price_range ~.,
                       df[train_index, ],
                       mtry = bagging.fit$bestTune[[1]],
                       splitrule = "gini",
                       min.node.size = bagging.fit$bestTune[[3]],
                       importance = "permutation",
                       scale.permutation.importance = TRUE)
barplot(sort(ranger::importance(bagging.final.per), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(20))

# Impurity based importance
set.seed(1)
bagging.final.imp <- ranger(price_range~ .,
                       df[train_index, ],
                       mtry = bagging.fit$bestTune[[1]],
                       splitrule = "gini",
                       min.node.size = bagging.fit$bestTune[[3]],
                       importance = "impurity")
barplot(sort(ranger::importance(bagging.final.imp), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan", "blue"))(20))
```
Since we have response variables, we consider the problem to be supervised learning. Therefore, the unsupervised learning methods (such as clustering and deep learning) are not suitable for this problem.

For the models we learned in the second half of this semester, we start with Bagging for classification problem. Bagging is a method which use bootstrap to reduce the variance. Though bagging can improve the predictive accuracy, it may be difficult to interpret since it's impossible to represent the results as a single tree. In the classification setting, we used Gini index to obtain the variable importance information, predictors with the larger total decrease in the Gini index are considered more important. Here, we use caret to do grid search, then choose the node size with the highest ROC value.


## Random forest

```{r }
ctrl <- trainControl(method = "cv", classProbs = TRUE, 
                     summaryFunction = twoClassSummary) 

rf.grid <- expand.grid(mtry = 1:8, 
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))
set.seed(1) 
rf.fit <- train(price_range ~ . , 
                df, 
                subset = train_index, 
                tuneGrid = rf.grid, 
                method = "ranger",
                metric = "ROC",
                trControl = ctrl)
```

```{r}
ggplot(rf.fit, highlight = TRUE)
```

After analyzing the variable importance in the bagging model, we realize that the variable `ram` is of great importance, which may cause bagging less valid in decreasing the variance since all of the bagged trees will look quite similar with top split at `ram`. Therefore, we fit a random forest model, which is an improved method of bagging by decorrelating the trees. It only consider a subset of predictors at each split so as to make the bagged trees less correlated with each other.


## AdaBoost

```{r }
gbm.grid <- expand.grid(n.trees = c(2000,3000,4000,5000), 
                        interaction.depth = 1:6, 
                        shrinkage = c(0.0005,0.001,0.002), 
                        n.minobsinnode = 1)
set.seed(1) 
gbm.fit <- train(price_range ~ . , 
                 df, 
                 subset = train_index, 
                 tuneGrid = gbm.grid, 
                 trControl = ctrl,
                 method = "gbm",
                 distribution = "adaboost",
                 metric = "ROC",
                 verbose = FALSE)
```

```{r}
ggplot(gbm.fit, highlight = TRUE)
```
Next, because the outcome of the mobile dataset is binary, we fit a gbm model to do boosting. Boosting is quite similar to bagging except it build trees using information from the previous tree. There are three tuning parameters in boosting: The number of trees B, which are selected using cross-validation; Shrinkage parameter $\lambda$ which controls the learning rate; And d, the number of splits in each tree which controls the complexity. From the result, we can see that the optimal situation is reached when B = 5000, $\lambda$ = 0.002, d = 5.


## SVM

```{r }
# Tune over both cost and sigma
# set.seed(1)
# svmr.fit <- train(price_range ~ . , 
#                   df, 
#                   subset = train_index, 
#                   method = "svmRadialSigma",
#                   tuneGrid = data.frame(C = exp(seq(-1,4,len=20)),
#                                         sigma = exp(seq(-6,-2,len=20))),
#                   trControl = ctrl,
#                   metric = "ROC",
#                   prob.model = TRUE,
#                   verbose = FALSE)
# myCol <- rainbow(20)
# myPar <- list(surpose.symbol = list(col = myCol),
#               surpose.line = list(col = myCol))
# plot(svmr.fit, highlight = TRUE, par.settings = myPar)


# Tune over cost
set.seed(1)
svm.fit = train(price_range ~ . , 
                df, 
                subset = train_index, 
                method = "svmRadialCost",
                tuneGrid = data.frame(C = exp(seq(-3,3,len=20))),
                trControl = ctrl,
                metric = "ROC",
                prob.model = TRUE,
                verbose = FALSE)
summary(svm.fit)
```

```{r}
plot(svm.fit)
```
Now consider to fit another classic classification model: support vector machine with radical kernel. There are two tuning parameters for SVM, cost and sigma. Here, we only tune on cost.


## ROC camparison

```{r}
pred.bagging = predict(bagging.fit, newdata = df[-train_index, ], type = "prob")[,1]
roc.bagging = pROC::roc(df$price_range[-train_index], pred.bagging)
pred.rf = predict(rf.fit, newdata = df[-train_index, ], type = "prob")[,1]
roc.rf = pROC::roc(df$price_range[-train_index], pred.rf)
pred.gbm = predict(gbm.fit, newdata = df[-train_index, ], type = "prob")[,1]
roc.gbm = pROC::roc(df$price_range[-train_index], pred.gbm)
pred.svm = predict(svm.fit, newdata = df[-train_index, ], type = "prob")[,1]
roc.svm = pROC::roc(df$price_range[-train_index], pred.svm)

plot(roc.bagging, col = 1) 
plot(roc.rf, add = TRUE, col = 2)
plot(roc.gbm, add = TRUE, col = 3)
plot(roc.svm, add = TRUE, col = 4)
auc <- c(roc.bagging$auc[1], roc.rf$auc[1], roc.gbm$auc[1], roc.svm$auc[1])
modelNames <- c("Bagging", "RF","Adaboost", "SVM") 
legend("bottomright", 
       legend = paste0(modelNames, ": ", 
                       round(auc,3)), 
       col = 1:4, lwd = 2)
```


## Global Importance

```{r}
gbmImp <- varImp(gbm.fit, scale = TRUE)
plot(gbmImp, top = 10)
```


## LIME

```{r fig.height=18, fig.width=12}
explainer.rf <- lime(df[train_index, -21], gbm.fit)
new_obs = df[-train_index, -21][1:6, ]
explaination.obs = explain(new_obs, 
                           explainer = explainer.rf,
                           n_features = 10,
                           n_labels = 2)
plot_features(explaination.obs)
```

## Prediction error

```{r}
pred.gbm.train = predict(gbm.fit, newdata = df[train_index, ], type = "prob")[, 1]
train_df$pred.gbm = pred.gbm.train
cp <- cutpointr(train_df, pred.gbm, price_range, 
                method = maximize_metric, metric = sum_sens_spec)
summary(cp)
```

```{r}
test_df$pred.gbm = as.factor(ifelse(pred.gbm > cp$optimal_cutpoint, "High", "Low"))
cft = confusionMatrix(test_df$pred.gbm, test_df$price_range)
print(cft)
```

* In order to predict the high cost phone, we decided to build a binary classification model. We randomly divided our dataset into two data sets before training the classification algorithms: the training and the test sets. The training and test sets each included 80% and 20% of the total data, respectively. 

* The parameters of each algorithm were determined based on the classification performance of the training set as measured by five-fold cross-validation. On the test set, the performance of all algorithms was tested and compared. We evaluated and compared the results of five different algorithms since different classification methods are better suited to different types of data. Bagging, random forest, ada boosting, and radical kernel SVM are among the models under consideration.

* We plotted the ROC curves of all the different algorithms on the test dataset. Over all reasonable sensitivity thresholds and recall thresholds, the ada boosting model is consistently better than all the other models. The feature importance of the ADA boosting model is scaled between 0 and 100. Random access memory (RAM) is the most important predictor since the importance value goes to 100. Battery power is 20% as important as RAM. Pixel height and pixel width are each around 10% as important as RAM. All the other predictors are less than 5% as important as RAM.

* For the first six test cases and label combinations, we utilized LIME to visually represent the explanations for the relationship between mobile phone price level and features. Positively associated features are displayed in blue, while negatively correlated features are displayed in red. For example, case 30, which refers to the row 30 of the test data, has the highest explanation fit 0.40. Label which is high means this case is for predicting the high price mobile phone. 'RAM smaller than 1209' feature which is red color implies the phone with this 'RAM smaller than 1209' feature has large possibility that it does not belong to the high price phone. For case 18, label which is high is for predicting the high price mobile phone. 'RAM > 3033' feature which is blue color implies the phone with this  'RAM > 3033' feature has large possibility that it belongs to high price phone. We can also observe that all of the predictors for the phone pricing outcome selected the same features, showing that these are important features both locally and globally. For example, these features include ram, battery power, talking time, pixel height, pixel width, 3g internet, WiFi, Bluetooth.

* We selected to maximize the sum of sensitivity and specificity in order to determine the best cut point for the prediction probability. On the training dataset, the best cut point is `r cp$optimal_cutpoint`, which yields an accuracy of `r cp$acc`, a sensitivity of `r cp$sensitivity`, and a specificity of `r cp$specificity`. On the test dataset, we have an accuracy of `r cft$overall[1]`, sensitivity of `r cft$byClass[1]`, and specificity of `r cft$byClass[2]` using the optimal cut point. As a consequence, our model appears to be extremely effective in predicting high prices for phones.







