---
title: "XIAO Midterm Report"
author: "XIAO MA"
date: "5/8/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Introduction 
In this digital era, mobile phones are no longer just a communication tool but have become a necessity in our daily life. Cell phone prices are from hundreds to thousands, and what is the major difference between these mobile. Our motivation for this project is to understand what the features such as RAM, 4G are relates to the price of the mobile. Relations between features of a mobile phone and its selling price (low or high). This project can help us to have a better understanding of mobile selling prices to better choose a cost-effective phone. It could also give the mobile company an idea of which mobile feature they need to work on. The data are collected on cellphone sales from multiple companies. There are 2000 rows and 21 columns. 
How did you prepare and clean the data? 
The raw data looks good does not need too many cleaning steps. Firstly, I convert the categorical variable as factors. They are `blue`, `dual_sim`,`four_g`,`touch_screen`,`wifi`. Then I deleted the categorical variable `three_g`(third generation of wireless mobile telecommunication technology)”, since we have another variable `four_g`, and with the development of technology, most cell phones already have 5G. Therefore, it is a little bit out of date to discuss 3G.  For the outcome variable `price_range`, has four price ranges defined as “low”, “medium”, “high” and “very high`.  I converted into a new binary response named `price` with “low” or “high” since we only focus on the relationship between features and price low or high rather than a specific price. Then delete the original y, `price_range`.
After cleaning the data, I split the whole dataset as 70% training data and 30% testing. The training data has 1400 rows and 20 variables, and the testing has 600 rows and 20 variables. There is no missing data.
Exploratory analysis/visualization
There are 700 observations in low price class and 700 observations in high price class. in the training subset. For categorical variables, their number of 0s or 1s are rough evenly distributed. According to Figure 1, we observed that the for variables `ram`, `talk_time`, `pc`, `fc`, `px_height`, `px_width`, `battery_power` and `int_memory` influence the outcome variable price, because the red line and thew blue dash line are not overlapped which indicates the difference. There is no obvious correlation between each variable, neither does multicollinearity. I also run the PCA (Table1) to check the importance of components. PC1, PC2, and PC3 have a higher influence on the response variable. The raw data looks good does not need too many cleaning steps. Firstly, I convert the categorical variable as factors. They are `blue`, `dual_sim`,`four_g`,`touch_screen`,`wifi`. Then I deleted the categorical variable `three_g`(third generation of wireless mobile telecommunication technology)”, since we have another variable `four_g`, and with the development of technology, most cell phones already have 5G. Therefore, it is a little bit out of date to discuss 3G.  For the outcome variable `price_range`, has four price ranges defined as “low”, “medium”, “high” and “very high`.  I converted into a new binary response named `price` with “low” or “high” since we only focus on the relationship between features and price low or high rather than a specific price. Then delete the original y, `price_range`. 
After cleaning the data, I split the whole dataset as 70% training data and 30% testing. The training data has 1400 rows and 20 variables, and the testing has 600 rows and 20 variables. There is no missing data.
Models
The outcome variable `price` is the binary response, hence six models have been chosen to fit the data. They are GLM (logistic regression), GLMN (penalized logistic regression), LDA, QDA, MARS, and GAM generalized additive models. For logistic regression, it does not require a linear relationship between predictors and responses variables. Logistic regression assumes that there exists a linear relationship between each predictor and the logit of the response variable. Both LDA and QDA are based on the Bayes theorem. LDA assumes data is Gaussian with a bell shape, and it assumes attribute has the same variance, that values of each variable vary around the mean by the same amount on average. QDA is similar to that. GAM and MARS models assume the nonlinear relationship between the dependent variable and covariates. I include all the predictors to fit the model. For tuning parameters, I choose degree = 1:4 and the nprune is 2:20 since there are 19 predictors for the MARS model.  For the confusion matrix, I just choose 0.5 as the cutoff point. 
The training and testing performance could be get from the confusion matrix. Firstly, I fit the logistic model, and the confusion matrix of testing data offers an accuracy is 0.985, which indicates that the fraction of correct prediction (295+296)/(295+4+5+296) is 98.5% with a 95% confidence interval of (0.972, 0.993). Kappa = 0.97. The No Information Rate is 0.5, which is the fraction of the "high" class in both predicted and trained datasets. We should reject the null hypothesis by checking the p-value, since the p-value is less than 0.05, and conclude that accuracy > no information rate. The confusion matrix of training data has higher accuracy than testing data which is 1 with 95% CI(0.997, 1). The other statistics are similar.
From the logistic model, we observed that `battery_power`, `mobile_wt`,`n_cores`, `px-heights`, `px_width` are significant variables since their p-value is less than p=0.05.According to the MARS model plot Figure2, we also observed that there are 4 important variables, and six variables do not go into the MARS model. Variable `ram` is the most important variable. The variables ` battery_power`, `px_height` `px_width` also played an important role in predicting the price of the mobile. Px_heights and px-width are pixel resolution height and width respectively. 
After fitting the 6 models, we used the resamples function to compare the performance of these models. We also get the ROC curves and the AUC values of each model. By observing the ROC curve and the boxplot, we believed that GLMN penalized logistic regression has a better performance among these six models. AUC for these six models are GLM 0.99, GLMN 0.999,LDA 0.995,QDA 0.993,GAM 0.991,MARS 0.998, are all pretty high. The only difference among these six models is the third digits after the decimal, which indicates the model has a pretty good performance and a 99% chance that the model able to figure out the right class.

Limitations  

The dataset may be old since in the raw dataset 50% of the mobile does not have 3G, but nowadays plenty of people already have 5G cell phones. Therefore, it may be a little out of date. Although the data have great performance on predicting price there is a little bit meaningless to discuss recently. It could offer an idea of the relationship between price and mobile features contemporarily. If the dataset could include more trending predictors to help the company create a more accurate model. The y is a 4-class response variable it could be more accurate if we fit the method of classification tree or multinomial regression.  From my point of view, one of the reasons why the model has such high accuracy is because I set the price_range to binary response which there is a more vague boundary, the predictors are not that sensitive to price as it is 4-class. The models might be overfitted and there may be a problem with coverage of GAM models since it always takes almost 40 mins to fit the model.

Conclusions

In conclusion, the GLMN penalized logistic regression model has the best performance with the AUC= 0.999 and the bestTune is at 102, lambda is 0.00046. `battery_power`, `px_height` `px_width`, `ram`, ` mobile_wt`, `n_cores` plays an important role in predicting the mobile price. Especially Ram, Pixel Resolution Height, and width, if these two values are high, the mobile got more chance to class as high price. We observed the partial dependence plot, when `ram` the Random Access Memory in MegaBytes greater than 3000, the price rise rapidly. When the Pixel Resolution Height is greater than 1780, the price rises rapidly. It reflects that these parts are the selling points of the mobile, with better memory storage and better Pixel Resolution, the customer could love to pay for it.


